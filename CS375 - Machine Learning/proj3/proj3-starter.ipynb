{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e56e29b-6e7e-4781-b7fb-906d3e768580",
   "metadata": {},
   "source": [
    "# Project 3: Language detection\n",
    "\n",
    "In this project, you will create a 2-layer neural network by hand to detect\n",
    "what language a sentence is written in.  I have provided you a data set\n",
    "containing sentences in English, Spanish, French, German, Italian, and Portuguese,\n",
    "and your neural network will use the frequency of the letters in the sentence\n",
    "to detect the language.  \n",
    "\n",
    "To make life simple, your network will only process two languages at a time,\n",
    "for example, English-vs-Spanish; we do this so we can create a network that\n",
    "does binary classification rather than multi-category classification.\n",
    "\n",
    "## Features\n",
    "\n",
    "The features we will use in this project are the relative frequencies of the\n",
    "26 letters of the English alphabet in each sentence.  Each of the six languages\n",
    "in our data set uses letters differently; for instance, see here:\n",
    "\n",
    "http://letterfrequency.org/letter-frequency-by-language/\n",
    "\n",
    "For example, while both Spanish and English have \"e\" as their most common letter,\n",
    "the relative popularities of other letters differ in small or large ways.  Take a\n",
    "look at the letter \"t\": in English, it's the 2nd-most-common letter, but in Spanish,\n",
    "it barely makes the top third.  \"h\" is another letter that is used more frequently in English\n",
    "than in Spanish.  The letters \"w\" and \"k\" show the opposite pattern: these letters appear\n",
    "very rarely in Spanish but are common in English.\n",
    "\n",
    "**Caveat**: While all six languages use essentially the same letters, there are extra\n",
    "letters (plus accents) that we will have to handle.  Though linguistically questionable,\n",
    "we will use a Python library to convert any letter not in the English alphabet to the closest\n",
    "English letter.  For instance, in Spanish, \"á\" will be converted to \"a,\" \"ñ\" will be converted\n",
    "to \"n,\" etc.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset contains a selection of 200 sentences in each of the six languages (1200 sentences in all).\n",
    "Each sentence is between 20 and 200 characters long.  **There is much more data available, but running this\n",
    "on individual laptops limits our ability to process more data.  Running your code on larger training sets\n",
    "will get you bonus points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3f631-4098-4914-ba1f-23148980f2c8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's examine our data set a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2226b780-a240-45eb-972b-38c6ce45a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d40713-5354-47e9-8b85-5dd050ab26ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>Sie ging über die sieben Berge zu den sieben Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es ist der gleiche Esel, aber ein anderer Sattel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deu</td>\n",
       "      <td>Darf ich an deinem Rechner meine Netzpost nach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wie viel macht sieben mal drei?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Sie hat ihn schreien hören.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>spa</td>\n",
       "      <td>Me recuerdo lo que vi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>spa</td>\n",
       "      <td>No hablo francés. Tampoco hablo inglés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>spa</td>\n",
       "      <td>Seguí así, que lograrás muchas cosas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>spa</td>\n",
       "      <td>Es mejor estar con gente inteligente en el inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>spa</td>\n",
       "      <td>Me lo encontré mientras estaba en Japón.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text\n",
       "0     deu  Sie ging über die sieben Berge zu den sieben Z...\n",
       "1     deu  Es ist der gleiche Esel, aber ein anderer Sattel.\n",
       "2     deu  Darf ich an deinem Rechner meine Netzpost nach...\n",
       "3     deu                    Wie viel macht sieben mal drei?\n",
       "4     deu                        Sie hat ihn schreien hören.\n",
       "...   ...                                                ...\n",
       "1195  spa                             Me recuerdo lo que vi.\n",
       "1196  spa            No hablo francés. Tampoco hablo inglés.\n",
       "1197  spa              Seguí así, que lograrás muchas cosas.\n",
       "1198  spa  Es mejor estar con gente inteligente en el inf...\n",
       "1199  spa           Me lo encontré mientras estaba en Japón.\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "\n",
    "all_data = pd.read_csv(\"six-languages.csv\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0f1879-3471-4820-a93d-6b7b18b42ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>deu</td>\n",
       "      <td>Königin Elisabeth II hat während ihres Lebens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>deu</td>\n",
       "      <td>Tom hat eine Menge Schriften von Maria.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>deu</td>\n",
       "      <td>Sie sind schwer bewaffnet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>deu</td>\n",
       "      <td>Er sieht so als, als wäre er krank gewesen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>deu</td>\n",
       "      <td>Schlagt euer Buch auf Seite 59 auf.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                               text\n",
       "56   deu  Königin Elisabeth II hat während ihres Lebens ...\n",
       "188  deu            Tom hat eine Menge Schriften von Maria.\n",
       "39   deu                         Sie sind schwer bewaffnet.\n",
       "74   deu        Er sieht so als, als wäre er krank gewesen.\n",
       "70   deu                Schlagt euer Buch auf Seite 59 auf."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>eng</td>\n",
       "      <td>Tom is better than me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>eng</td>\n",
       "      <td>Tom propped himself up on one elbow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>eng</td>\n",
       "      <td>Ziri treated Rima with respect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>eng</td>\n",
       "      <td>The clientele seemed satisfied.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>eng</td>\n",
       "      <td>My sister has been sick.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                  text\n",
       "229  eng                Tom is better than me.\n",
       "260  eng  Tom propped himself up on one elbow.\n",
       "293  eng       Ziri treated Rima with respect.\n",
       "203  eng       The clientele seemed satisfied.\n",
       "337  eng              My sister has been sick."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>fra</td>\n",
       "      <td>Ils sont déjà arrivés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tu n'aurais pas dû venir si tôt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>fra</td>\n",
       "      <td>À en juger d'après ce superbe bronzage, il sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>fra</td>\n",
       "      <td>N'avez-vous rien de mieux à faire ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>fra</td>\n",
       "      <td>J'ai l'intention d'entrer en contact.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                               text\n",
       "556  fra                             Ils sont déjà arrivés.\n",
       "485  fra                   Tu n'aurais pas dû venir si tôt.\n",
       "436  fra  À en juger d'après ce superbe bronzage, il sem...\n",
       "468  fra                N'avez-vous rien de mieux à faire ?\n",
       "502  fra              J'ai l'intention d'entrer en contact."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>ita</td>\n",
       "      <td>Che succhi di frutta avete?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>ita</td>\n",
       "      <td>Lei ha visto quel film, vero?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>ita</td>\n",
       "      <td>Io non sono creativo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>ita</td>\n",
       "      <td>È stato molto gentile da parte tua di prestarm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>ita</td>\n",
       "      <td>L'artista è eternamente un ragazzo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                               text\n",
       "752  ita                        Che succhi di frutta avete?\n",
       "623  ita                      Lei ha visto quel film, vero?\n",
       "761  ita                              Io non sono creativo.\n",
       "685  ita  È stato molto gentile da parte tua di prestarm...\n",
       "656  ita                L'artista è eternamente un ragazzo."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>por</td>\n",
       "      <td>Estou fazendo uma boneca para Anna.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>por</td>\n",
       "      <td>Este campo é pobre em água.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>por</td>\n",
       "      <td>Os irmãos estão andando na praia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>por</td>\n",
       "      <td>Em um breve momento de genialidade, eu descobr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>por</td>\n",
       "      <td>Você me considera uma ameaça, não é mesmo?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                               text\n",
       "975  por                Estou fazendo uma boneca para Anna.\n",
       "811  por                        Este campo é pobre em água.\n",
       "860  por                  Os irmãos estão andando na praia.\n",
       "908  por  Em um breve momento de genialidade, eu descobr...\n",
       "840  por         Você me considera uma ameaça, não é mesmo?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>spa</td>\n",
       "      <td>En ese momento me pareció una buena idea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>spa</td>\n",
       "      <td>El calor se apaga automáticamente cuando la ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>spa</td>\n",
       "      <td>Tú eres lista, pero yo también.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>spa</td>\n",
       "      <td>¿A qué hora llegó ella al Aeropuerto de Narita?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>spa</td>\n",
       "      <td>Tomás trabajaba con María.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text\n",
       "1144  spa          En ese momento me pareció una buena idea.\n",
       "1093  spa  El calor se apaga automáticamente cuando la ha...\n",
       "1171  spa                    Tú eres lista, pero yo también.\n",
       "1163  spa    ¿A qué hora llegó ella al Aeropuerto de Narita?\n",
       "1063  spa                         Tomás trabajaba con María."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a few sentences from each language (at random).\n",
    "# The languages are denoted by three-letter abbreviations:\n",
    "\n",
    "# deu = German\n",
    "# eng = English\n",
    "# fra = French\n",
    "# ita = Italian\n",
    "# por = Portuguese\n",
    "# spa = Spanish\n",
    "\n",
    "LANGUAGES = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    display(all_data[all_data['lang'] == lang].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4344b-c485-497b-9179-fe86f3fdba90",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Here, we will remove punctuation, spaces, convert everything to lowercase, and\n",
    "convert all characters to the 26-letter English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b2e796-e158-4038-a873-98e215bf46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before modifying:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>deu</td>\n",
       "      <td>Der harte Winter hat seine Spuren hinterlassen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>eng</td>\n",
       "      <td>I like to go to the park and watch the childre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a embrassé son chien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ita</td>\n",
       "      <td>Gli uomini d'affari vanno spesso a questo rist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>spa</td>\n",
       "      <td>Por favor hable más alto.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text\n",
       "59    deu    Der harte Winter hat seine Spuren hinterlassen.\n",
       "259   eng  I like to go to the park and watch the childre...\n",
       "459   fra                          Tom a embrassé son chien.\n",
       "659   ita  Gli uomini d'affari vanno spesso a questo rist...\n",
       "859   por       Não gosto de misturar negócios com diversão.\n",
       "1059  spa                          Por favor hable más alto."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After converting:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>text_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>deu</td>\n",
       "      <td>Der harte Winter hat seine Spuren hinterlassen.</td>\n",
       "      <td>derhartewinterhatseinespurenhinterlassen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>eng</td>\n",
       "      <td>I like to go to the park and watch the childre...</td>\n",
       "      <td>iliketogototheparkandwatchthechildrenintheplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a embrassé son chien.</td>\n",
       "      <td>tomaembrassesonchien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ita</td>\n",
       "      <td>Gli uomini d'affari vanno spesso a questo rist...</td>\n",
       "      <td>gliuominidaffarivannospessoaquestoristorante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "      <td>naogostodemisturarnegocioscomdiversao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>spa</td>\n",
       "      <td>Por favor hable más alto.</td>\n",
       "      <td>porfavorhablemasalto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text  \\\n",
       "59    deu    Der harte Winter hat seine Spuren hinterlassen.   \n",
       "259   eng  I like to go to the park and watch the childre...   \n",
       "459   fra                          Tom a embrassé son chien.   \n",
       "659   ita  Gli uomini d'affari vanno spesso a questo rist...   \n",
       "859   por       Não gosto de misturar negócios com diversão.   \n",
       "1059  spa                          Por favor hable más alto.   \n",
       "\n",
       "                                             text_alpha  \n",
       "59             derhartewinterhatseinespurenhinterlassen  \n",
       "259   iliketogototheparkandwatchthechildrenintheplay...  \n",
       "459                                tomaembrassesonchien  \n",
       "659        gliuominidaffarivannospessoaquestoristorante  \n",
       "859               naogostodemisturarnegocioscomdiversao  \n",
       "1059                               porfavorhablemasalto  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>text_alpha</th>\n",
       "      <th>freq_a</th>\n",
       "      <th>freq_b</th>\n",
       "      <th>freq_c</th>\n",
       "      <th>freq_d</th>\n",
       "      <th>freq_e</th>\n",
       "      <th>freq_f</th>\n",
       "      <th>freq_g</th>\n",
       "      <th>freq_h</th>\n",
       "      <th>freq_i</th>\n",
       "      <th>freq_j</th>\n",
       "      <th>freq_k</th>\n",
       "      <th>freq_l</th>\n",
       "      <th>freq_m</th>\n",
       "      <th>freq_n</th>\n",
       "      <th>freq_o</th>\n",
       "      <th>freq_p</th>\n",
       "      <th>freq_q</th>\n",
       "      <th>freq_r</th>\n",
       "      <th>freq_s</th>\n",
       "      <th>freq_t</th>\n",
       "      <th>freq_u</th>\n",
       "      <th>freq_v</th>\n",
       "      <th>freq_w</th>\n",
       "      <th>freq_x</th>\n",
       "      <th>freq_y</th>\n",
       "      <th>freq_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>deu</td>\n",
       "      <td>Der harte Winter hat seine Spuren hinterlassen.</td>\n",
       "      <td>derhartewinterhatseinespurenhinterlassen</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>eng</td>\n",
       "      <td>I like to go to the park and watch the childre...</td>\n",
       "      <td>iliketogototheparkandwatchthechildrenintheplay...</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a embrassé son chien.</td>\n",
       "      <td>tomaembrassesonchien</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ita</td>\n",
       "      <td>Gli uomini d'affari vanno spesso a questo rist...</td>\n",
       "      <td>gliuominidaffarivannospessoaquestoristorante</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "      <td>naogostodemisturarnegocioscomdiversao</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>spa</td>\n",
       "      <td>Por favor hable más alto.</td>\n",
       "      <td>porfavorhablemasalto</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text  \\\n",
       "59    deu    Der harte Winter hat seine Spuren hinterlassen.   \n",
       "259   eng  I like to go to the park and watch the childre...   \n",
       "459   fra                          Tom a embrassé son chien.   \n",
       "659   ita  Gli uomini d'affari vanno spesso a questo rist...   \n",
       "859   por       Não gosto de misturar negócios com diversão.   \n",
       "1059  spa                          Por favor hable más alto.   \n",
       "\n",
       "                                             text_alpha    freq_a  freq_b  \\\n",
       "59             derhartewinterhatseinespurenhinterlassen  0.075000    0.00   \n",
       "259   iliketogototheparkandwatchthechildrenintheplay...  0.057143    0.00   \n",
       "459                                tomaembrassesonchien  0.100000    0.05   \n",
       "659        gliuominidaffarivannospessoaquestoristorante  0.113636    0.00   \n",
       "859               naogostodemisturarnegocioscomdiversao  0.081081    0.00   \n",
       "1059                               porfavorhablemasalto  0.200000    0.05   \n",
       "\n",
       "        freq_c    freq_d    freq_e    freq_f    freq_g    freq_h    freq_i  \\\n",
       "59    0.000000  0.025000  0.200000  0.000000  0.000000  0.075000  0.075000   \n",
       "259   0.028571  0.042857  0.128571  0.000000  0.042857  0.085714  0.071429   \n",
       "459   0.050000  0.000000  0.150000  0.000000  0.000000  0.050000  0.050000   \n",
       "659   0.000000  0.022727  0.068182  0.045455  0.022727  0.000000  0.113636   \n",
       "859   0.054054  0.054054  0.081081  0.000000  0.054054  0.000000  0.081081   \n",
       "1059  0.000000  0.000000  0.050000  0.050000  0.000000  0.050000  0.000000   \n",
       "\n",
       "        freq_j    freq_k    freq_l    freq_m    freq_n    freq_o    freq_p  \\\n",
       "59    0.000000  0.000000  0.025000  0.000000  0.125000  0.000000  0.025000   \n",
       "259   0.014286  0.028571  0.057143  0.014286  0.085714  0.071429  0.028571   \n",
       "459   0.000000  0.000000  0.000000  0.100000  0.100000  0.100000  0.000000   \n",
       "659   0.000000  0.000000  0.022727  0.022727  0.090909  0.113636  0.022727   \n",
       "859   0.000000  0.000000  0.000000  0.054054  0.054054  0.189189  0.000000   \n",
       "1059  0.000000  0.000000  0.100000  0.050000  0.000000  0.150000  0.050000   \n",
       "\n",
       "        freq_q    freq_r    freq_s    freq_t    freq_u    freq_v    freq_w  \\\n",
       "59    0.000000  0.125000  0.100000  0.100000  0.025000  0.000000  0.025000   \n",
       "259   0.000000  0.042857  0.028571  0.100000  0.014286  0.014286  0.014286   \n",
       "459   0.000000  0.050000  0.150000  0.050000  0.000000  0.000000  0.000000   \n",
       "659   0.022727  0.068182  0.113636  0.068182  0.045455  0.022727  0.000000   \n",
       "859   0.000000  0.081081  0.108108  0.054054  0.027027  0.027027  0.000000   \n",
       "1059  0.000000  0.100000  0.050000  0.050000  0.000000  0.050000  0.000000   \n",
       "\n",
       "      freq_x    freq_y  freq_z  \n",
       "59       0.0  0.000000     0.0  \n",
       "259      0.0  0.028571     0.0  \n",
       "459      0.0  0.000000     0.0  \n",
       "659      0.0  0.000000     0.0  \n",
       "859      0.0  0.000000     0.0  \n",
       "1059     0.0  0.000000     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove all punctuation from the sentences, and convert all characters\n",
    "# to the 26-letter English alphabet.\n",
    "# We do this by first removing all characters from each sentence that \n",
    "\n",
    "ALL_LETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# Illustrate with some data.\n",
    "# Print one example from each language before modifying.\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Before modifying:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data[all_data['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)\n",
    "    \n",
    "all_data_freq = all_data.copy()\n",
    "\n",
    "# Remove punctuation & spaces, convert to lowercase, convert to 26-letter English alphabet.\n",
    "\n",
    "all_data_freq['text_alpha'] = all_data_freq['text'].map(lambda str: unidecode(\"\".join(c for c in str.lower() if c.isalpha())))\n",
    "print(\"\\nAfter converting:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data_freq[all_data_freq['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)\n",
    "    \n",
    "# Find letter frequencies.\n",
    "\n",
    "for letter in ALL_LETTERS:\n",
    "    all_data_freq['freq_' + letter] = all_data_freq['text_alpha'].map(lambda str: str.lower().count(letter)/len(str))\n",
    "    \n",
    "print(\"With frequencies:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data_freq[all_data_freq['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5f522-f152-4af6-b72f-eaa33585f9bc",
   "metadata": {},
   "source": [
    "## Part A: Single layer neural network\n",
    "\n",
    "Here, you will write code to create a single layer neural network that can distinguish between two\n",
    "languages of your choice (from the six options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21d9138-84a9-4cce-b97a-5badf3acfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Part A.\n",
    "\n",
    "# Choose your languages.  Edit the variables below to set one language\n",
    "# to be the \"positive class\" (LANG1) and one to be the \"negative class\" (LANG0).\n",
    "# Use the three-letter abbreviation: deu, eng, fra, ita, por, spa.\n",
    "\n",
    "LANG1 = 'spa'  # positive category\n",
    "LANG0 = 'eng'  # negative category\n",
    "\n",
    "# SANITY CHECKS BELOW ARE GIVEN FOR SPANISH/ENGLISH, but after you make sure those\n",
    "# two languages are working, you can play around with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2522912c-11b8-4866-a703-0db890bb2647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>freq_a</th>\n",
       "      <th>freq_b</th>\n",
       "      <th>freq_c</th>\n",
       "      <th>freq_d</th>\n",
       "      <th>freq_e</th>\n",
       "      <th>freq_f</th>\n",
       "      <th>freq_g</th>\n",
       "      <th>freq_h</th>\n",
       "      <th>freq_i</th>\n",
       "      <th>freq_j</th>\n",
       "      <th>freq_k</th>\n",
       "      <th>freq_l</th>\n",
       "      <th>freq_m</th>\n",
       "      <th>freq_n</th>\n",
       "      <th>freq_o</th>\n",
       "      <th>freq_p</th>\n",
       "      <th>freq_q</th>\n",
       "      <th>freq_r</th>\n",
       "      <th>freq_s</th>\n",
       "      <th>freq_t</th>\n",
       "      <th>freq_u</th>\n",
       "      <th>freq_v</th>\n",
       "      <th>freq_w</th>\n",
       "      <th>freq_x</th>\n",
       "      <th>freq_y</th>\n",
       "      <th>freq_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang    freq_a    freq_b    freq_c    freq_d    freq_e    freq_f  \\\n",
       "200      0  0.111111  0.000000  0.000000  0.083333  0.083333  0.000000   \n",
       "201      0  0.078947  0.026316  0.052632  0.026316  0.078947  0.052632   \n",
       "202      0  0.000000  0.000000  0.000000  0.083333  0.125000  0.000000   \n",
       "203      0  0.037037  0.000000  0.037037  0.074074  0.296296  0.037037   \n",
       "204      0  0.060606  0.000000  0.000000  0.030303  0.151515  0.000000   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1195     1  0.000000  0.000000  0.058824  0.058824  0.235294  0.000000   \n",
       "1196     1  0.125000  0.062500  0.062500  0.000000  0.062500  0.031250   \n",
       "1197     1  0.166667  0.000000  0.066667  0.000000  0.066667  0.000000   \n",
       "1198     1  0.045455  0.000000  0.030303  0.000000  0.212121  0.015152   \n",
       "1199     1  0.121212  0.030303  0.030303  0.000000  0.181818  0.000000   \n",
       "\n",
       "        freq_g    freq_h    freq_i    freq_j    freq_k    freq_l    freq_m  \\\n",
       "200   0.000000  0.055556  0.000000  0.000000  0.027778  0.000000  0.055556   \n",
       "201   0.052632  0.026316  0.105263  0.000000  0.000000  0.105263  0.026316   \n",
       "202   0.000000  0.125000  0.125000  0.000000  0.041667  0.083333  0.041667   \n",
       "203   0.000000  0.037037  0.111111  0.000000  0.000000  0.074074  0.037037   \n",
       "204   0.000000  0.060606  0.060606  0.000000  0.030303  0.030303  0.060606   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1195  0.000000  0.000000  0.058824  0.000000  0.000000  0.058824  0.058824   \n",
       "1196  0.031250  0.062500  0.031250  0.000000  0.000000  0.093750  0.031250   \n",
       "1197  0.066667  0.033333  0.066667  0.000000  0.000000  0.033333  0.033333   \n",
       "1198  0.030303  0.000000  0.075758  0.015152  0.000000  0.045455  0.015152   \n",
       "1199  0.000000  0.000000  0.030303  0.030303  0.000000  0.030303  0.060606   \n",
       "\n",
       "        freq_n    freq_o    freq_p    freq_q    freq_r    freq_s    freq_t  \\\n",
       "200   0.083333  0.138889  0.000000  0.000000  0.083333  0.000000  0.083333   \n",
       "201   0.052632  0.078947  0.000000  0.000000  0.000000  0.000000  0.184211   \n",
       "202   0.041667  0.083333  0.041667  0.000000  0.000000  0.041667  0.083333   \n",
       "203   0.037037  0.000000  0.000000  0.000000  0.000000  0.111111  0.111111   \n",
       "204   0.060606  0.090909  0.000000  0.000000  0.030303  0.060606  0.212121   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1195  0.000000  0.117647  0.000000  0.058824  0.117647  0.000000  0.000000   \n",
       "1196  0.093750  0.156250  0.031250  0.000000  0.031250  0.062500  0.031250   \n",
       "1197  0.000000  0.066667  0.000000  0.033333  0.066667  0.200000  0.000000   \n",
       "1198  0.151515  0.106061  0.015152  0.015152  0.060606  0.060606  0.090909   \n",
       "1199  0.151515  0.090909  0.030303  0.000000  0.060606  0.060606  0.090909   \n",
       "\n",
       "        freq_u    freq_v    freq_w  freq_x    freq_y  freq_z  \n",
       "200   0.027778  0.000000  0.055556     0.0  0.111111     0.0  \n",
       "201   0.026316  0.000000  0.026316     0.0  0.000000     0.0  \n",
       "202   0.041667  0.041667  0.000000     0.0  0.000000     0.0  \n",
       "203   0.000000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "204   0.030303  0.000000  0.000000     0.0  0.030303     0.0  \n",
       "...        ...       ...       ...     ...       ...     ...  \n",
       "1195  0.117647  0.058824  0.000000     0.0  0.000000     0.0  \n",
       "1196  0.000000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "1197  0.100000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "1198  0.015152  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "1199  0.000000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "\n",
       "[400 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I have written some code to produce a data set from the one above with only\n",
    "# our two chosen languages.  It also removes the full sentences (which we don't need),\n",
    "# since we're only training on the \"freq_\" columns.\n",
    "\n",
    "def make_dataset(complete_data, pos_cat, neg_cat):\n",
    "    data = complete_data.copy()\n",
    "    data = data.drop(columns=['text', 'text_alpha'])\n",
    "    data = data[data['lang'].isin([pos_cat, neg_cat])]\n",
    "    data['lang'] = (data['lang'] == pos_cat).astype(int)\n",
    "    return data\n",
    "\n",
    "data = make_dataset(all_data_freq, LANG1, LANG0)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab60c839-e90b-47a3-b200-14953a849918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 26)\n",
      "(200, 26)\n",
      "(200,)\n",
      "(200,)\n",
      "(array([0, 1]), array([100, 100], dtype=int64))\n",
      "(array([0, 1]), array([100, 100], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "## Create testing and training sets.\n",
    "\n",
    "# In the real world, we would shuffle the data to make a random training/testing split,\n",
    "# but here, since we have 200 examples, we're going to use the first half of each\n",
    "# language for training and the last half for testing.\n",
    "\n",
    "# In our data set above, the first half of the examples are one class and the last half\n",
    "# are the other class (although depending on the languages you chose, 0 might be first and 1\n",
    "# second, or the other way around, though it doesn't matter).  \n",
    "\n",
    "# So create a training set of rows 0-99 and 200-299,\n",
    "# and a testing set of rows 100-199 and 300-399\n",
    "# \n",
    "\n",
    "# Make training set:\n",
    "X = data.copy().drop(columns=['lang'])  # get rid of the lang column\n",
    "y = data.copy()['lang']  # keep only the lang column\n",
    "                      \n",
    "X_train = pd.concat([X.iloc[0:100], X.iloc[200:300]]).to_numpy()\n",
    "X_test = pd.concat([X.iloc[100:200], X.iloc[300:400]]).to_numpy()\n",
    "y_train = pd.concat([y.iloc[0:100], y.iloc[200:300]]).to_numpy()\n",
    "y_test = pd.concat([y.iloc[100:200], y.iloc[300:400]]).to_numpy()\n",
    "\n",
    "# Sanity checks.\n",
    "print(X_train.shape)  # should be (200, 26)\n",
    "print(X_test.shape) # should be (200, 26)\n",
    "print(y_train.shape) # should be (200,)\n",
    "print(y_test.shape) # should be (200,)\n",
    "print(np.unique(y_train, return_counts=True)) # count the number of 0's and 1's, should be 100 of each\n",
    "print(np.unique(y_test, return_counts=True)) # count the number of 0's and 1's, should be 100 of each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e1002-79eb-4c98-b20c-6f648c90ff2d",
   "metadata": {},
   "source": [
    "## Single Layer Neural Network Refresher\n",
    "\n",
    "Recall that in a single layer network, we have the following variables.  Some variables have their superscripts\n",
    "dropped because in a single layer network, there's only one copy of them.\n",
    "\n",
    "- $x$: values of our features that are in the input to the network.  Here, this\n",
    "  is a vector of 26 letter frequencies.  In the code, this is called `x`.\n",
    "- $in^{[1]}$: A copy of $x$, but with a 1 appended at the front.  In code, this is called `in` or `in1`.\n",
    "- $W$ or $W^{[1]}$: Weight matrix. Dimensions are 1 row by 27 columns, because we have 27 features and 1 output\n",
    "  in our network.  In code, this is `W`.\n",
    "- $z$ or $z^{[1]}$: Computed as the matrix product of $W$ and the inputs $in^{[1]}$.  Because $W$ is only one row, this is \n",
    "  essentially the dot product of $W$ (treated as a vector) and $in^{[1]}$.  In code, this is `z` or `z1`.\n",
    "- $a$ or $a^{[1]}$: Computed by the sigmoid function (activation function) applied to $z^{[1]}$.\n",
    "  In code, this is `a` or `a1`.\n",
    "  \n",
    "Recall that a neural network's output is the result of the last activation function, which here is $a$ or $a^{[1]}$.\n",
    "\n",
    "The loss function for a neural network doing classification is the cross-entropy loss function, which is:\n",
    "\n",
    "$L(\\hat{y}, y) = \\dfrac{\\hat{y} - y}{\\hat{y}  (1 - \\hat{y})}$.\n",
    "\n",
    "Because the predicted value of a neural net, $\\hat{y}$ is the same as the output variable $a$ or $a^{[1]}$ here,\n",
    "we will often see:\n",
    "\n",
    "$L(a, y) = \\dfrac{a - y}{a  (1 - a)}$.\n",
    "\n",
    "The cost function for a neural net is just the average loss over an entire set of training examples:\n",
    "\n",
    "$J(W) = \\displaystyle \\dfrac{1}{m}\\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$,\n",
    "\n",
    "where $y^{(i)}$ is the $i$'th training example *correct* output (0 or 1) and $\\hat{y}^{(i)} = a^{(i)}$ is the \n",
    "corresponding *predicted* output (a number between 0 and 1) for that same $i$'th training example.\n",
    "\n",
    "The forward propagation code for this neural net is already written for you.  Note that it returns\n",
    "three variables, `in`, `z1`, and `a1`.\n",
    "  \n",
    "You must write the following functions:\n",
    "\n",
    "- `compute_cost`: Compute the cost (average loss) over the training set.\n",
    "- `backward_prop`: Run backpropagation.\n",
    "\n",
    "The values you are computing for backpropagation are the entries in the matrix $\\dfrac{\\partial L}{\\partial w_j}$.\n",
    "We are using the notation $w_j$ to stand for the $j$'th entry in the weight matrix $W$.  Because $W$ has only one row,\n",
    "$w_j$ is stored in the variable `W[0][j]` and the corresponding partial derivative will be in `dL_dW[0][j]`.\n",
    "\n",
    "Formula: $\\dfrac{\\partial L}{\\partial w_j} = \\dfrac{\\partial L}{\\partial a} \\cdot \\dfrac{\\partial a}{\\partial z}\n",
    "  \\cdot \\dfrac{\\partial z}{\\partial w_j}$\n",
    "  \n",
    "where\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial a} = \\dfrac{a-y}{a(1-a)}$\n",
    "\n",
    "$\\dfrac{\\partial a}{\\partial z} = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "and\n",
    "\n",
    "$\\dfrac{\\partial z}{\\partial w_j} = in_j$\n",
    "\n",
    "So you will return a single row, 27 column matrix with entries using the formula above (three multiplications).\n",
    "You can do this with a for loop.\n",
    "Note that the first two terms are the same for all entries in the matrix, and so don't need to be computed inside the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ae7557-0abb-42cb-8c28-7f9684107123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer neural network\n",
    "\n",
    "INPUT_SIZE = 27\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z(W, inputs):\n",
    "    '''\n",
    "    Compute the z vector for a neural network.\n",
    "    inputs: inputs to the neural network, vector of variable size.\n",
    "    W: weight matrix for the inputs, matrix of variable size.\n",
    "    '''\n",
    "    return W @ inputs\n",
    "\n",
    "def compute_activation(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def deriv_cross_entropy_loss(a, y):\n",
    "    return (a - y)/(a * (1 - a))\n",
    "\n",
    "def compute_cross_entropy_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W, x):\n",
    "    '''\n",
    "    Run the forward propagation algorithm.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    returns: 2 scalars, z1 and a1\n",
    "    '''\n",
    "    DEBUG = False\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    # set inputs\n",
    "    in1 = augment_vector(x)\n",
    "    if DEBUG: print(\"Input to this layer is\", in1)\n",
    "    \n",
    "    # make sure dimensions match inputs and W\n",
    "    assert len(in1) == W.shape[1]\n",
    "\n",
    "    # get W matrix\n",
    "    if DEBUG: print(\"Using W of shape\", W.shape)\n",
    "\n",
    "    # compute z1\n",
    "    z1 = compute_z(W, in1)\n",
    "    if DEBUG: print(\"z is\", z1)\n",
    "\n",
    "    # compute a1\n",
    "    a1 = compute_activation(z1)\n",
    "    if DEBUG: print(\"a1 is\", a1)\n",
    "    \n",
    "    return in1, z1, a1\n",
    "    \n",
    "def compute_output(x, W):\n",
    "    '''\n",
    "    Returns the output of the neural network (probability of x being in the 1 class).\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    in1, z1, a1 = forward_prop(W, x)\n",
    "    return a1\n",
    "\n",
    "def make_prediction(x, W):\n",
    "    '''\n",
    "    Returns the classification of x.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    in1, z1, a1 = forward_prop(W, x)\n",
    "    if a1 >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_cost(X_data, y_data, W):\n",
    "    '''\n",
    "    Compute the cost (average loss) over the training examples in X_data, y_data.\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    returns: a scalar with the cost\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    m = X_data.shape[0]\n",
    "    return sum([compute_cross_entropy_loss(compute_output(X_data[i], W), y_data[i]) for i in range(m)]) / m\n",
    "\n",
    "def backward_prop(W, y, in1, z1, a1):\n",
    "    '''\n",
    "    Run backward propagation.\n",
    "    W: weight matrix of size (1, INPUT_SIZE)\n",
    "    y: scalar of correct target output, 0 or 1\n",
    "    in1: inputs to the neural network, length INPUT_SIZE\n",
    "    z1: z1 scalar from forward prop of NN\n",
    "    a1: activation vector from forward prop of NN\n",
    "    Returns: partial derivatives of loss function with\n",
    "       respect to each entry in weight matrix W (same dimensions as W)\n",
    "    '''\n",
    "    \n",
    "    # make sure dimensions match inputs and W\n",
    "    assert len(in1) == W.shape[1]\n",
    "    \n",
    "    # make derivative matrix of same size as W, filled with zeros\n",
    "    dL_dW = np.full_like(W, 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            dL_dW[i][j] = deriv_cross_entropy_loss(a1[i], y) * deriv_activation(z1) * in1[j]\n",
    "    \n",
    "    return dL_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d502f34a-8180-494e-88fa-ba76d62bc1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
      " 0.         0.         0.05555556 0.         0.         0.02777778\n",
      " 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
      " 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
      " 0.         0.11111111 0.        ] \n",
      " [2.] \n",
      " [0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for forward_prop\n",
    "\n",
    "W_sanity = np.ones((1, INPUT_SIZE))\n",
    "in1, z1, a1 = forward_prop(W_sanity, X_train[0])\n",
    "print(in1, \"\\n\", z1, \"\\n\", a1)\n",
    "\n",
    "# output:\n",
    "#[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
    "# 0.         0.         0.05555556 0.         0.         0.02777778\n",
    "# 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
    "# 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
    "# 0.         0.11111111 0.        ] \n",
    "# [2.] \n",
    "# [0.88079708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8250a99f-c586-4e6e-a562-4b6728857800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12692801])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks for compute_cost\n",
    "\n",
    "compute_cost(X_train, y_train, W_sanity)  # should be array([1.12692801])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b18092cc-5c8e-4ebc-84b7-03cef5ef7f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88079708 0.09786634 0.         0.         0.07339976 0.07339976\n",
      "  0.         0.         0.04893317 0.         0.         0.02446659\n",
      "  0.         0.04893317 0.07339976 0.12233293 0.         0.\n",
      "  0.07339976 0.         0.07339976 0.02446659 0.         0.04893317\n",
      "  0.         0.09786634 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for backprop\n",
    "\n",
    "print(backward_prop(W_sanity, y_train[0], in1, z1, a1))\n",
    "\n",
    "#[[0.88079708 0.09786634 0.         0.         0.07339976 0.07339976\n",
    "#  0.         0.         0.04893317 0.         0.         0.02446659\n",
    "#  0.         0.04893317 0.07339976 0.12233293 0.         0.\n",
    "#  0.07339976 0.         0.07339976 0.02446659 0.         0.04893317\n",
    "#  0.         0.09786634 0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92bafc3b-775e-48f6-b503-86d567305319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W: [[ -5.02347478  29.64641395  -0.56908367  16.24691417   1.45151702\n",
      "   14.62875781  -3.5911138   -5.75869304 -25.58081249  -6.28873739\n",
      "    1.08170268 -12.80143375   5.5865951   -6.75871979   3.16963092\n",
      "   11.70538821   4.08028965  19.26492954   1.04764947   9.00421624\n",
      "  -25.88882025  29.1215695    1.82916821 -30.30426785   1.73440939\n",
      "  -14.19332804   2.11238345]]\n"
     ]
    }
   ],
   "source": [
    "# Write code for gradient descent here.\n",
    "\n",
    "# This is the same as every other gradient descent algorithm you've\n",
    "# written.  Use a loop to run each iteration of gradient descent.\n",
    "# Each iteration will make a pass over the training set X_train, y_train,\n",
    "# running forward prop & backward prop to get the output of the network\n",
    "# and the gradient matrix dL_dW.  We need to compute the AVERAGE of all\n",
    "# these dL_dW matrices (we compute one for each training example), so you\n",
    "# will need to add up the matrices as you calculate them into a \"sum\" or\n",
    "# \"total\" matrix variable.  Then once the loop over the training set is done,\n",
    "# you compute:\n",
    "# W -= alpha * (1/m) * (sum of all the dL_dW matrices).\n",
    "\n",
    "# Note that you can comment out the W = np.ones((1, INPUT_SIZE)) line \n",
    "# and the J_sequence = [] line\n",
    "# if you run gradient descent but then want to run the cell again to \n",
    "# run additional training iterations.  Commenting out those initialization lines\n",
    "# just lets you pick up where you left off when you ran the cell before.\n",
    "\n",
    "W = np.ones((1, INPUT_SIZE))  \n",
    "ALPHA = 20 # change this\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "for i in range(1000):\n",
    "    W -= ALPHA * (1 / X_train.shape[0]) * sum([backward_prop(W, y_train[i], *forward_prop(W, X_train[i])) for i in range(X_train.shape[0])])\n",
    "    J_sequence.append(compute_cost(X_train, y_train, W))\n",
    "    \n",
    "print(\"Final W:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18d536d2-b691-459a-a42f-200ca575475e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2529afca-0a10-41ee-aaed-79079e2df21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "# Test/train accuracy\n",
    "\n",
    "def compute_accuracy(X, y):\n",
    "    # YOUR CODE HERE - compute accuracy for this data set of examples (X) and their\n",
    "    # corresponding outputs (y).  X is a matrix and y is a vector, but X has the same\n",
    "    # number of rows as y has entries.  Hint:\n",
    "    m = X.shape[0]\n",
    "    y_hat = np.array([make_prediction(X[i], W) for i in range(m)])\n",
    "    return np.sum(y_hat == y) / m\n",
    "    \n",
    "\n",
    "# Try to get training accuracy and test accuracy to at least\n",
    "# 95% and 90% respectively.  (for Spanish/English)\n",
    "\n",
    "print(compute_accuracy(X_train, y_train))\n",
    "print(compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "615d9397-df8a-459e-8639-52a58e508e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Do you know why Tom and Mary aren't here today?\" classified as eng\n",
      "\"Los taxis en China son gratuitos, solo tienes que correr rápido.\" classified as spa\n",
      "\"Tom can be a little difficult to get along with.\" classified as eng\n",
      "\"Déjame dar una vuelta en tu Toyota nuevo.\" classified as spa\n",
      "\"I think I should've helped Tom.\" classified as eng\n",
      "\"La casa se encuentra en medio de árboles.\" classified as spa\n",
      "\"The clientele seemed satisfied.\" classified as eng\n",
      "\"Cuando como mucho por la noche tengo pesadillas.\" classified as spa\n",
      "\"It only takes me three minutes to do that.\" classified as eng\n",
      "\"No voy a dejar que hagas eso.\" classified as spa\n",
      "\n",
      "You typed Cuando como mucho por la noche tengo pesadillas\n",
      "Predicted language: spa\n"
     ]
    }
   ],
   "source": [
    "# Real life demo!\n",
    "\n",
    "def classify_language(sentence):\n",
    "    stripped_sentence = unidecode(\"\".join(c for c in sentence.lower() if c.isalpha()))\n",
    "    #print(\"Removed punctuation/accents/spaces:\", stripped_sentence)\n",
    "    d = {}\n",
    "    for letter in ALL_LETTERS:\n",
    "        d['freq_' + letter] = [stripped_sentence.lower().count(letter)/len(stripped_sentence)]\n",
    "    x_demo = pd.DataFrame(d).to_numpy()[0]\n",
    "    answer = make_prediction(x_demo, W)\n",
    "    if answer == 1:\n",
    "        return LANG1\n",
    "    else:\n",
    "        return LANG0\n",
    "    \n",
    "# Demo first five examples of each language:\n",
    "for i in range(0, 5):\n",
    "    sentence = all_data[all_data['lang'] == LANG0]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    sentence = all_data[all_data['lang'] == LANG1]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    \n",
    "print()\n",
    "sentence = input(\"Type in a sentence in \" + LANG0 + \" or \" + LANG1)\n",
    "print(\"You typed\", sentence)\n",
    "\n",
    "print(\"Predicted language:\", classify_language(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f76d4-d051-4ec0-8105-cd8c0a17baad",
   "metadata": {},
   "source": [
    "## Part B: 2 Layer Neural Network Refresher\n",
    "\n",
    "Recall that in a 2 layer network (1 hidden layer), we have the following variables.  \n",
    "\n",
    "- `HIDDEN_LAYER_SIZE`: number of nodes in the hidden layer.  This can be set to essentially any integer greater than zero.\n",
    "  Larger numbers will generally increase performance, but will take longer to train.\n",
    "- $x$: values of our features that are in the input to the network.  Here, this\n",
    "  is a vector of 26 letter frequencies.  In the code, this is called `x`.\n",
    "- $in^{[1]}$: A copy of $x$, but with a 1 appended at the front.  In code, this is called `in1`.\n",
    "- $W^{[1]}$: Weight matrix. Dimensions are HIDDEN_LAYER_SIZE rows by 27 columns, because the first layer has 27 input \n",
    "  features and HIDDEN_LAYER_SIZE outputs.  In code, this is `W1`.\n",
    "- $z^{[1]}$: Computed as the matrix product of $W1$ and the inputs $in^{[1]}$.  This will be a vector with the\n",
    "  same number of entries as HIDDEN_LAYER_SIZE, because the first layer is now computing this many outputs (rather than\n",
    "  just 1).  In code, this is `z1`.\n",
    "- $a$ or $a^{[1]}$: Computed by the sigmoid function (activation function) applied to $z^{[1]}$.  Same size as $a^{[1]}$\n",
    "  In code, this is `a` or `a1`.\n",
    "- $in^{[2]}$: A copy of $a^{[1]}$, but with a 1 appended at the front.  In code, this is called `in2`.\n",
    "- $W^{[2]}$: Weight matrix. Dimensions are 1 rows by HIDDEN_LAYER_SIZE+1 columns, because the second layer has \n",
    "  HIDDEN_LAYER_SIZE inputs (one each from layer 1, plus the bias input) and 1 output.  In code, this is `W2`.\n",
    "- $z^{[2]}$: Computed as the matrix product of $W2$ and the inputs $in^{[2]}$.  Because $W2$ is only one row, this is \n",
    "  essentially the dot product of $W2$ (treated as a vector) and $in^{[2]}$.  In code, this is `z2`.\n",
    "- $a^{[2]}$: Computed by the sigmoid function (activation function) applied to $z^{[2]}$.  Final output of the network.\n",
    "  In code, this is `a2`.\n",
    "\n",
    "Recall that a 2-layer neural network's output is the result of the last activation function, which here is $a^{[2]}$.\n",
    "  \n",
    "You must write the following functions:\n",
    "\n",
    "- `forward_prop`: Run forward propagation.  You can use the single-layer NN code as a guide.  Returns six \n",
    "  variables: in1, a1, z1, in2, z2, a2.\n",
    "- `make_prediction`: Use the single-layer code as a guide.\n",
    "- `compute_cost`: Compute the cost (average loss) over the training set.\n",
    "- `backward_prop`: Run backpropagation.\n",
    "\n",
    "The values you are computing for backpropagation are the entries in the TWO matrices $\\dfrac{\\partial L}{\\partial w^{[2]}_j}$\n",
    "and $\\dfrac{\\partial L}{\\partial w^{[1]}_{k,j}}$.\n",
    "\n",
    "We are using the notation $w^{[2]}_j$ to stand for the $j$'th entry in the weight matrix $W^{[2]}$.  Because $W^{[2]}$ has only one row,\n",
    "$w^{[2]}_j$ is stored in the variable `W2[0][j]` and the corresponding partial derivative will be in `dL_dW2[0][j]`.\n",
    "\n",
    "We are using the notation $w^{[1]}_{k,j}$ to stand for the $k$'th row and $j$'th column in the weight matrix $W^{[1]}$.  \n",
    "Unlike $W^{[2]}$, $W^{[1]}$ has multiple rows, so \n",
    "$w^{[1]}_{k,j}$ is stored in the variable `W1[k][j]` and the corresponding partial derivative will\n",
    "be in `dL_dW1[k][j]`.\n",
    "\n",
    "Formulas: \n",
    "\n",
    "Derivatives of output layer going backwards to hidden layer (do this first).  These entries will go in `dL_dW2`.\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w^{[2]}_j} = \\dfrac{\\partial L}{\\partial a^{[2]}} \\cdot \\dfrac{\\partial a^{[2]}}{\\partial z^{[2]}}\n",
    "  \\cdot \\dfrac{\\partial z^{[2]}}{\\partial w^{[2]}_j}$\n",
    "  \n",
    "this is all basically the same as the single layer network, with one change:\n",
    "\n",
    "$\\dfrac{\\partial z^{[2]}}{\\partial w^{[2]}_j} = in^{[2]}_j$  (we just added the [2] subscript)\n",
    "\n",
    "So you will return a single row, HIDDEN_LAYER_SIZE column matrix with entries using the formula above (three multiplications).\n",
    "You can do this with a for loop where you loop over the `in2` variable.\n",
    "Note that the first two terms are the same for all entries in the matrix, and so don't need to be computed inside the loop.\n",
    "\n",
    "The derivatives for going from the hidden layer to the input layer are slightly more complicated.\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w^{[1]}_{k,j}} = \\dfrac{\\partial L}{\\partial a^{[2]}} \\cdot \\dfrac{\\partial a^{[2]}}{\\partial z^{[2]}}\n",
    "  \\cdot \\dfrac{\\partial z^{[2]}}{\\partial a^{[1]}_k} \\cdot \\dfrac{\\partial a^{[1]}_k}{\\partial z^{[1]}_k}\n",
    "  \\cdot \\dfrac{\\partial z^{[1]}_k}{\\partial w^{[1]}_{k,j}}$. \n",
    "  \n",
    "The first two terms are the same as the first two terms above.\n",
    "\n",
    "The third, fourth, and fifth terms are defined as:\n",
    "\n",
    "$\\dfrac{\\partial z^{[2]}}{\\partial a^{[1]}_k} = w^{[2]}_{k+1}$ = `W2[0][k+1]` (remember W2 is only one row) (the k+1 comes\n",
    "from the fact that when we moved from $a^{[1]}$ to $in^{[2]}$, we added a bias input as $in^{[2]}_0$, so all the subscripts \n",
    "for $in^{[2]}$ are shifted up by 1.\n",
    "\n",
    "$\\dfrac{\\partial a^{[1]}_k}{\\partial z^{[1]}_k} = \\sigma(z^{[1]}_k)(1-\\sigma(z^{[1]}_k)$.\n",
    "\n",
    "$\\dfrac{\\partial z^{[1]}_k}{\\partial w^{[1]}_{k,j}} = in^{[1]}_j$\n",
    "\n",
    "Computing this whole thing:\n",
    "\n",
    "Use nested loops, one for $j$ and one for $k$.  Which one is inner and which one is outer doesn't matter too much.\n",
    "The $k$ variable counts *rows* of `W1`/`dL_dW1` and $j$ counts the columns.  So you can calculate the upper bounds\n",
    "of these nested loops in a few ways, based on the dimensions of `W1` or the lengths of `in1` and `in2`:\n",
    "\n",
    "  In other words, remember the dimensions of `W1` (and therefore `dL_dW1`) are HIDDEN_LAYER_SIZE rows by 27 columns,\n",
    "  So $k$ (row counter) should range from 0 to `HIDDEN_LAYER_SIZE` which should also be `len(in2)-1`.\n",
    "  And $j$ (column counter) should range from 0 to `len(in1)` which should be 27.\n",
    "  \n",
    "Inside the nested loop, you should compute the last three terms of the five above.  You can re-use the first\n",
    "two terms from the `dL_dW2` computation, assuming you saved them in variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "150065bb-9f1e-41bf-9b54-c63ea09b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer neural network (one hidden layer)\n",
    "\n",
    "INPUT_SIZE = 27\n",
    "HIDDEN_LAYER_SIZE = 5\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z(W, inputs):\n",
    "    '''\n",
    "    Compute the z vector for a neural network.\n",
    "    inputs: inputs to the neural network, vector of variable size.\n",
    "    W: weight matrix for the inputs, matrix of variable size.\n",
    "    '''\n",
    "    return W @ inputs\n",
    "\n",
    "def compute_activation(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def deriv_cross_entropy_loss(a, y):\n",
    "    return (a - y)/(a * (1 - a))\n",
    "\n",
    "def compute_cross_entropy_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W1, W2, x):\n",
    "    '''\n",
    "    Run the forward propagation algorithm.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    returns: in1, z1, a1, in2, z2, a2\n",
    "    '''\n",
    "    DEBUG = False\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE: use single layer NN code as a guide.\n",
    "    \n",
    "    # compute in1\n",
    "    in1 = augment_vector(x)\n",
    "    \n",
    "    # compute z1\n",
    "    z1 = compute_z(W1, in1)\n",
    "    if DEBUG: print(\"z is\", z1)\n",
    "\n",
    "    # compute a1\n",
    "    a1 = compute_activation(z1)\n",
    "    if DEBUG: print(\"a1 is\", a1)\n",
    "    \n",
    "    # compute in2\n",
    "    in2 = augment_vector(a1)\n",
    "    \n",
    "    # compute z2\n",
    "    z2 = compute_z(W2, in2)\n",
    "    \n",
    "    # compute a2\n",
    "    a2 = compute_activation(z2)\n",
    "    \n",
    "    return in1, z1, a1, in2, z2, a2\n",
    "    \n",
    "def compute_output(x, W1, W2):\n",
    "    '''\n",
    "    Returns the output of the neural network (probability of x being in the 1 class).\n",
    "    x: input to the neural network, vector of size INPUT_SIZE\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    in1, z1, a1, in2, z2, a2 = forward_prop(W1, W2, x)\n",
    "    return a2\n",
    "    \n",
    "def make_prediction(x, W1, W2):\n",
    "    '''\n",
    "    Returns the classification of x.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    _, _, _, _, _, a2 = forward_prop(W1, W2, x)\n",
    "    if a2 >= 0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def compute_cost(X_data, y_data, W1, W2):\n",
    "    '''\n",
    "    Compute the cost (average loss) over the training examples in X_data, y_data.\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    m = X_data.shape[0]\n",
    "    return np.sum([compute_cross_entropy_loss(compute_output(X_data[i], W1, W2), y_data[i]) for i in range(m)]) / m\n",
    "\n",
    "def backward_prop(W1, W2, y, in1, z1, a1, in2, z2, a2):\n",
    "    '''\n",
    "    Run backward propagation.\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    y: scalar of correct target output, 0 or 1\n",
    "    in1: inputs to the neural network, length INPUT_SIZE\n",
    "    z1: z1 scalar from forward prop of NN\n",
    "    a1: activation vector from forward prop of NN\n",
    "    in2: inputs to the 2nd layer of NN\n",
    "    z2: z2 scalar from forward prop of NN\n",
    "    a2: activation vector from forward prop of NN\n",
    "    Returns: partial derivatives of loss function with\n",
    "        respect to each entry in weight matrix W1 and W2 (same dimensions as W1 and W2)\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # make derivative matrix of same size as W1, filled with zeros\n",
    "    dL_dW1 = np.full_like(W1, 0)\n",
    "    \n",
    "    # make derivative matrix of same size as W2, filled with zeros\n",
    "    dL_dW2 = np.full_like(W2, 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            dL_dW1[i][j] = deriv_cross_entropy_loss(a2, y) * deriv_activation(z2) * W2[0][i+1] * deriv_activation(z1[i]) * in1[j]\n",
    "    \n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            dL_dW2[i][j] = deriv_cross_entropy_loss(a2[i], y) * deriv_activation(z2) * in2[j]\n",
    "    \n",
    "    return dL_dW1, dL_dW2\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104007c1-b8f7-440b-a190-95b177986d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
      " 0.         0.         0.05555556 0.         0.         0.02777778\n",
      " 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
      " 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
      " 0.         0.11111111 0.        ] \n",
      " [2. 2. 2. 2. 2.] \n",
      " [0.88079708 0.88079708 0.88079708 0.88079708 0.88079708]\n",
      "[1.         0.88079708 0.88079708 0.88079708 0.88079708 0.88079708] \n",
      " [5.40398539] \n",
      " [0.99552153]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for forward_prop (for a hidden layer size of 5)\n",
    "\n",
    "W1_sanity = np.ones((HIDDEN_LAYER_SIZE, INPUT_SIZE))\n",
    "W2_sanity = np.ones((1, HIDDEN_LAYER_SIZE+1))\n",
    "in1, z1, a1, in2, z2, a2 = forward_prop(W1_sanity, W2_sanity, X_train[0])\n",
    "print(in1, \"\\n\", z1, \"\\n\", a1)\n",
    "print(in2, \"\\n\", z2, \"\\n\", a2)\n",
    "\n",
    "# output:\n",
    "#[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
    "# 0.         0.         0.05555556 0.         0.         0.02777778\n",
    "# 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
    "# 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
    "# 0.         0.11111111 0.        ] \n",
    "# [2. 2. 2. 2. 2.] \n",
    "# [0.88079708 0.88079708 0.88079708 0.88079708 0.88079708]\n",
    "#[1.         0.88079708 0.88079708 0.88079708 0.88079708 0.88079708] \n",
    "# [5.40398539] \n",
    "# [0.99552153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d322e3-1258-491a-a9d8-7803d5af3cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.70648122])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks for compute_cost (hidden layer size = 5)\n",
    "\n",
    "compute_cost(X_train, y_train, W1_sanity, W2_sanity)  # should be array([2.70648122])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70884ee9-5408-445b-96c2-7a556dc0b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
      "        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
      "        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
      "        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
      "        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
      "        0.01161371, 0.        ],\n",
      "       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
      "        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
      "        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
      "        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
      "        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
      "        0.01161371, 0.        ],\n",
      "       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
      "        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
      "        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
      "        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
      "        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
      "        0.01161371, 0.        ],\n",
      "       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
      "        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
      "        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
      "        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
      "        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
      "        0.01161371, 0.        ],\n",
      "       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
      "        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
      "        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
      "        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
      "        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
      "        0.01161371, 0.        ]]), array([[0.99552153, 0.87685246, 0.87685246, 0.87685246, 0.87685246,\n",
      "        0.87685246]]))\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for backprop (hidden layer size = 5)\n",
    "\n",
    "print(backward_prop(W1_sanity, W2_sanity, y_train[0], in1, z1, a1, in2, z2, a2))\n",
    "\n",
    "#(array([[0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ]]), array([[0.99552153, 0.87685246, 0.87685246, 0.87685246, 0.87685246,\n",
    "#        0.87685246]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d26477e-ef28-4227-9c4d-16e95ae8ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W1 and W2: [[-2.72890661e+00 -5.11965723e-01  2.57019680e-01  3.75150167e-02\n",
      "  -1.79362411e-01  2.55102305e-01 -2.94514069e-01  1.22782347e+00\n",
      "  -1.24607832e+00 -3.75035674e-01 -6.27936858e-01  1.08560666e+00\n",
      "  -4.02897543e-01 -2.67596203e-01 -1.37610880e+00 -7.93039198e-02\n",
      "   9.62207130e-01  6.53703218e-01 -8.40742267e-02 -7.45499929e-01\n",
      "  -6.37352432e-01 -6.39379241e-01 -6.48599763e-01 -1.48355341e+00\n",
      "  -6.50270804e-02 -1.85245046e-01 -2.48409586e+00]\n",
      " [ 1.32993153e+00 -8.68010660e+00 -6.27141690e-01 -6.27906295e+00\n",
      "  -9.60683504e-01 -4.09407548e+00  4.74883585e-01  9.50300032e-01\n",
      "   6.71270587e+00  3.45752938e+00  1.97849970e+00  5.57029621e+00\n",
      "  -1.09960436e+00  1.21144452e+00 -2.53218940e-01 -6.65387704e+00\n",
      "   7.81553213e-01 -9.97343949e+00  3.41690287e-01 -3.66468707e+00\n",
      "   7.71847271e+00 -1.16971502e+01 -1.02078785e+00  1.34619188e+01\n",
      "  -9.24918870e-01  3.16418868e+00  6.50700005e-01]\n",
      " [ 3.37433680e-01 -8.61262631e+00  5.04438361e-01 -3.66337733e+00\n",
      "  -1.20379198e+00 -4.20752822e+00  2.04973657e+00  1.53848509e+00\n",
      "   5.53256764e+00  9.46471444e-01 -1.54345175e+00  3.54671830e+00\n",
      "  -9.55634584e-01  1.60547357e+00 -1.74245764e+00 -2.27562074e+00\n",
      "  -1.17948203e+00 -5.70903445e+00  1.94610548e+00 -2.63698771e+00\n",
      "   7.07268267e+00 -6.29628995e+00  4.64796299e-01  9.33163114e+00\n",
      "   3.26129670e-01  4.79699965e+00 -1.22219370e+00]\n",
      " [-1.58189489e+00  1.07452903e+01 -1.43460002e+00  5.72537901e+00\n",
      "   6.98563032e-01  5.57166740e+00 -1.86241697e+00 -8.20191203e-01\n",
      "  -9.48631445e+00 -2.89595414e+00 -9.08306146e-02 -4.79921583e+00\n",
      "   1.67166092e+00 -2.55529702e+00 -1.70147376e+00  2.56209575e+00\n",
      "   1.78298205e+00  9.09062148e+00 -4.12386599e-01  4.66998855e+00\n",
      "  -8.72270565e+00  1.23111338e+01  9.72229927e-02 -1.38362035e+01\n",
      "   6.02369207e-01 -6.40194277e+00  7.50286010e-02]\n",
      " [ 4.02382986e-01 -6.20776685e+00  8.29646276e-02 -5.35574257e+00\n",
      "  -6.07800670e-01 -4.06291696e+00  1.27128519e+00  2.83470316e+00\n",
      "   5.32926266e+00  2.77231246e+00 -4.78871042e-01  3.88788381e+00\n",
      "   1.15085421e+00  1.25073340e+00  3.43698053e-03 -4.60837480e+00\n",
      "   1.14516883e+00 -6.77920161e+00 -1.00116697e+00 -1.22199175e+00\n",
      "   6.76173119e+00 -8.30668946e+00 -2.02116374e+00  6.46484390e+00\n",
      "   8.30832296e-01  3.38641772e+00 -1.15985181e+00]] [[  3.00311859   0.57302373 -10.71618828  -7.50131278   9.63097338\n",
      "   -7.0277887 ]]\n"
     ]
    }
   ],
   "source": [
    "# Write code for gradient descent here.\n",
    "\n",
    "# This is the same as earlier in this project, except you have\n",
    "# two matrix variables, W1 and W2 now, and their corresponding\n",
    "# gradients (derivative variables).\n",
    "\n",
    "# Note that you can comment out the two W1/W2 initialization lines \n",
    "# and the J_sequence initialization line\n",
    "# if you run gradient descent but then want to run the cell again to \n",
    "# run additional training iterations.  Commenting out the W initialization line\n",
    "# just lets you pick up where you left off when you ran the cell before.\n",
    "\n",
    "# THIS WILL TAKE A LONG TIME TO CONVERGE.  WHERE LONG TIME = POSSIBLY 5-10 MINUTES.\n",
    "# I recommend finding a reasonable alpha by starting with a small number of iterations.\n",
    "# Once you have a good alpha, then you can start running more and more iterations\n",
    "# as to not have to sit around for minutes at a time waiting while this runs.\n",
    "# You can also use the commenting-out-the-initialization idea above to see\n",
    "# if you've converged, and if not, just pick up where you left off.\n",
    "\n",
    "W1 = np.random.normal(size=(HIDDEN_LAYER_SIZE, INPUT_SIZE))\n",
    "W2 = np.random.normal(size=(1, HIDDEN_LAYER_SIZE+1))\n",
    "ALPHA = 5.0\n",
    "J_sequence = []\n",
    "\n",
    "for i in range(2000):\n",
    "    dW = [backward_prop(W1, W2, y_train[i], *forward_prop(W1, W2, X_train[i])) for i in range(X_train.shape[0])]\n",
    "    W1 -= ALPHA * (1 / X_train.shape[0]) * sum([dW[i][0] for i in range(X_train.shape[0])])\n",
    "    W2 -= ALPHA * (1 / X_train.shape[0]) * sum([dW[i][1] for i in range(X_train.shape[0])])\n",
    "    J_sequence.append(compute_cost(X_train, y_train, W1, W2))\n",
    "    \n",
    "print(\"Final W1 and W2:\", W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abf48547-ae0e-4277-b0c5-b54d8a44946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "182a20c3-1abf-4225-a7af-7952058d9320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "# Test/train accuracy\n",
    "\n",
    "def compute_accuracy(X, y):\n",
    "    m = X.shape[0]\n",
    "    y_hat = np.array([make_prediction(X[i], W1, W2) for i in range(m)])\n",
    "    return np.sum(y_hat == y) / m\n",
    "    \n",
    "\n",
    "# Try to get training accuracy and test accuracy to at least\n",
    "# 99% and 91% respectively.  (for spanish/english)\n",
    "\n",
    "print(compute_accuracy(X_train, y_train))\n",
    "print(compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "645de7fd-d976-4d92-a7ec-260867e1a9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Do you know why Tom and Mary aren't here today?\" classified as eng\n",
      "\"Los taxis en China son gratuitos, solo tienes que correr rápido.\" classified as spa\n",
      "\"Tom can be a little difficult to get along with.\" classified as eng\n",
      "\"Déjame dar una vuelta en tu Toyota nuevo.\" classified as spa\n",
      "\"I think I should've helped Tom.\" classified as eng\n",
      "\"La casa se encuentra en medio de árboles.\" classified as spa\n",
      "\"The clientele seemed satisfied.\" classified as eng\n",
      "\"Cuando como mucho por la noche tengo pesadillas.\" classified as spa\n",
      "\"It only takes me three minutes to do that.\" classified as eng\n",
      "\"No voy a dejar que hagas eso.\" classified as spa\n",
      "\n",
      "You typed Los taxis en China son gratuitos, solo tienes que correr rápido.\n",
      "Predicted language: spa\n"
     ]
    }
   ],
   "source": [
    "# Real life demo!\n",
    "\n",
    "def classify_language(sentence):\n",
    "    stripped_sentence = unidecode(\"\".join(c for c in sentence.lower() if c.isalpha()))\n",
    "    #print(\"Removed punctuation/accents/spaces:\", stripped_sentence)\n",
    "    d = {}\n",
    "    for letter in ALL_LETTERS:\n",
    "        d['freq_' + letter] = [stripped_sentence.lower().count(letter)/len(stripped_sentence)]\n",
    "    x_demo = pd.DataFrame(d).to_numpy()[0]\n",
    "    answer = make_prediction(x_demo, W1, W2)\n",
    "    if answer == 1:\n",
    "        return LANG1\n",
    "    else:\n",
    "        return LANG0\n",
    "    \n",
    "# Demo first five examples of each language:\n",
    "for i in range(0, 5):\n",
    "    sentence = all_data[all_data['lang'] == LANG0]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    sentence = all_data[all_data['lang'] == LANG1]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    \n",
    "print()\n",
    "sentence = input(\"Type in a sentence in \" + LANG0 + \" or \" + LANG1)\n",
    "print(\"You typed\", sentence)\n",
    "\n",
    "print(\"Predicted language:\", classify_language(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33a322ab-f889-4272-90e7-38b2291b1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL QUESTIONS\n",
    "# answer this with respect to training on Spanish/English\n",
    "\n",
    "# Train/Test accuracy for single-layer network:  (copy from above)\n",
    "# ANSWER: 0.975/0.92\n",
    "\n",
    "# Train/Test accuracy for two-layer network:  (copy from above)\n",
    "# ANSWER: 0.99/0.92\n",
    "\n",
    "# Extra credit for testing other languages.  For each other language pair\n",
    "# you test, report training and testing accuracy.  (Up to 3 bonus points).\n",
    "# ANSWERS: (tell me what language pairs you tried)\n",
    "\n",
    "# For up to 2 more bonus points, speculate on why some language pairs are \"harder\"\n",
    "# to learn than others.  Harder = lower testing/training accuracies meaning\n",
    "# they are harder to distinguish from each other.  Refer to the specific\n",
    "# language pairs you ran and the accuracies you came up with to support your\n",
    "# arguments.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
